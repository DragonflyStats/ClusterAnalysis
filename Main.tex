\tableofcontents
% http://www.dataperspective.info/2013/12/cluster-analysis-using-r.html

\section{Cluster Analysis}

\subsection{Introduction to Cluster Analysis (optional)}


%----------------------------------------------%
% 1  Clustering Techniques

Much of the history of cluster analysis is concerned with developing algorithms that were not too computer intensive, since early computers were not nearly as powerful as they are today. Accordingly, computational shortcuts have traditionally been used in many cluster analysis algorithms. These algorithms have proven to be very useful, and can be found in most computer software.
More recently, many of these older methods have been revisited and updated to reflect the fact that certain computations that once would have overwhelmed the available computers can now be performed routinely. In R, a number of these updated versions of cluster analysis algorithms are available through the cluster library, providing us with a large selection of methods to perform cluster analysis, and the possibility of comparing the old methods with the new to see if they really provide an advantage.
One of the oldest methods of cluster analysis is known as k-means cluster analysis, and is available in R through the kmeans function. The first step (and certainly not a trivial one) when using k-means cluster analysis is to specify the number of clusters (k) that will be formed in the final solution. The process begins by choosing k observations to serve as centers for the clusters. Then, the distance from each of the other observations is calculated for each of the k clusters, and observations are put in the cluster to which they are the closest. After each observation has been put in a cluster, the center of the clusters is recalculated, and every observation is checked to see if it might be closer to a different cluster, now that the centers have been recalculated. The process continues until no observations switch clusters.
Looking on the good side, the k-means technique is fast, and doesn't require calculating all of the distances between each observation and every other observation. It can be written to efficiently deal with very large data sets, so it may be useful in cases where other methods fail. On the down side, if you rearrange your data, it's very possible that you'll get a different solution every time you change the ordering of your data. Another criticism of this technique is that you may try, for example, a 3 cluster solution that seems to work pretty well, but when you look for the 4 cluster solution, all of the structure that the 3 cluster solution revealed is gone. This makes the procedure somewhat unattractive if you don't know exactly how many clusters you should have in the first place.


\begin{itemize}
\item Cluster analysis is a convenient method for identifying homogenous groups of
objects called clusters. Objects (or cases, observations) in a specific cluster share
many characteristics, but are very dissimilar to objects not belonging to that cluster.
\item  There are three cluster analysis approaches: 
\begin{itemize}
  \item hierarchical methods,
  \item partitioning methods (more precisely, k-means), 
  item and two-step clustering,
\textit{essentially a combination of the first two methods.}
\end{itemize}
%In the last class we looked as hierarchical clustering analysis.
\item Each of these procedures
follows a different approach to grouping the most similar objects into a cluster and
to determining each object’s cluster membership. Software packages, such as \texttt{R} calculate a measure
of (dis)similarity by estimating the distance between pairs of objects. \\ Objects with
smaller distances between one another are more similar, whereas objects with larger
distances are more dissimilar.
\item Some approaches – most notably hierarchical methods – require us to specify how similar or different objects
    are in order to identify different clusters. 
\item An important problem in the application of cluster analysis is the decision
regarding how many clusters should be derived from the data. This question is
explored in the next step of the analysis. Sometimes, however,
number of segments that have to be derived from the data will be known in advance.
\item
By choosing a specific clustering procedure, we determine how clusters are to be
formed. This always involves optimizing some kind of criterion, such as minimizing
the within-cluster variance (i.e., the clustering variables’ overall variance of
objects in a specific cluster), or maximizing the distance between the objects or
clusters). The procedure could also address the question of how to determine the
(dis)similarity between objects in a newly formed cluster and the remaining objects
in the dataset.
\end{itemize}
\newpage
%--------------------------------------------------------------------%
\subsection{Implementation with \texttt{R}}
The R cluster library provides a modern alternative to k-means clustering, known as pam, which is an acronym for "Partitioning around Medoids". The term medoid refers to an observation within a cluster for which the sum of the distances between it and all the other members of the cluster is a minimum. pam requires that you know the number of clusters that you want (like k-means clustering), but it does more computation than k-means in order to insure that the medoids it finds are truly representative of the observations within a given cluster. Recall that in the k-means method the centers of the clusters (which might or might not actually correspond to a particular observation) are only recaculated after all of the observations have had a chance to move from one cluster to another. With pam, the sums of the distances between objects within a cluster are constantly recalculated as observations move around, which will hopefully provide a more reliable solution. Furthermore, as a by-product of the clustering operation it identifies the observations that represent the medoids, and these observations (one per cluster) can be considered a representative example of the members of that cluster which may be useful in some situations. pam does require that the entire distance matrix is calculated to facilitate the recalculation of the medoids, and it does involve considerably more computation than k-means, but with modern computers this may not be a important consideration. As with k-means, there's no guarantee that the structure that's revealed with a small number of clusters will be retained when you increase the number of clusters.
Another class of clustering methods, known as hierarchical agglomerative clustering methods, starts out by putting each observation into its own separate cluster. It then examines all the distances between all the observations and pairs together the two closest ones to form a new cluster. This is a simple operation, since hierarchical methods require a distance matrix, and it represents exactly what we want - the distances between individual observations. So finding the first cluster to form simply means looking for the smallest number in the distance matrix and joining the two observations that the distance correspnds to into a new cluster. Now there is one less cluster than there are observations. To determine which observations will form the next cluster, we need to come up with a method for finding the distance between an existing cluster and individual observations, since once a cluster has been formed, we'll determine which observation will join it based on the distance between the cluster and the observation. Some of the methods that have been proposed to do this are to take the minimum distance between an observation and any member of the cluster, to take the maximum distance, to take the average distance, or to use some kind of measure that minimizes the distances between observations within the cluster. Each of these methods will reveal certain types of structure within the data. Using the minimum tends to find clusters that are drawn out and "snake"-like, while using the maximum tends to find compact clusters. Using the mean is a compromise between those methods. One method that tends to produce clusters of more equal size is known as Ward's method. It attempts to form clusters keeping the distances within the clusters as small as possible, and is often useful when the other methods find clusters with only a few observations. Agglomerative Hierarchical cluster analysis is provided in R through the hclust function.
Notice that, by its very nature, solutions with many clusters are nested within the solutions that have fewer clusters, so observations don't "jump ship" as they do in k-means or the pam methods. Furthermore, we don't need to tell these procedures how many clusters we want - we get a complete set of solutions starting from the trivial case of each observation in a separate cluster all the way to the other trivial case where we say all the observations are in a single cluster.
Traditionally, hierarchical cluster analysis has taken computational shortcuts when updating the distance matrix to reflect new clusters. In particular, when a new cluster is formed and the distance matrix is updated, all the information about the individual members of the cluster is discarded in order to make the computations faster. The cluster library provides the agnes function which uses essentially the same technique as hclust, but which uses fewer shortcuts when updating the distance matrix. For example, when the mean method of calculating the distance between observations and clusters is used, hclust only uses the two observations and/or clusters which were recently merged when updating the distance matrix, while agnes calculates those distances as the average of all the distances between all the observations in the two clusters. While the two techniques will usually agree quite closely when minimum or maximum updating methods are used, there may be noticeable differences when updating using the average distance or Ward's method.
%----------------------------------------------%

\subsubsection{Application Areas}

\newpage
\subsection{Key Concepts}


\subsubsection{Supervised Learning v Unsupervised Learning}

\subsubsection{Agglomerative vs Divisive Clustering}
\subsubsection{Distance Matrix}

%----------------------------------------------%

\subsection{Types of Hierarchical Clustering}
\begin{itemize}
\item
Hierarchical clustering procedures are characterized by the tree-like structure
established in the course of the analysis. Most hierarchical techniques fall into a
category called agglomerative clustering. In this category, clusters are consecutively
formed from objects. 
\item Initially, this type of procedure starts with each object
representing an individual cluster. These clusters are then sequentially merged
according to their similarity. First, the two most similar clusters (i.e., those with
the smallest distance between them) are merged to form a new cluster at the bottom
of the hierarchy. In the next step, another pair of clusters is merged and linked to a
higher level of the hierarchy, and so on. This allows a hierarchy of clusters to be
established from the bottom up.
\item A cluster hierarchy can also be generated top-down. In this divisive clustering,
all objects are initially merged into a single cluster, which is then gradually split up. 
\item Divisive procedures are quite rarely used in practice. We therefore
concentrate on the agglomerative clustering procedures.
\item A consequence of the Hierarchical Clustering method is that that if an object is assigned
to a certain cluster, there is no possibility of reassigning this object to another
cluster. This is an important distinction between these types of clustering and
partitioning methods such as \textbf{\textit{k-means}}.
\end{itemize}



%-------------------------------------------------------------------%
\newpage
\section{AGNES: Agglomerative Nesting}

As an example of using the agnes function from the cluster package, consider the famous Fisher iris data, available as the dataframe iris in R. First let's look at some of the data:
\begin{framed}
\begin{verbatim}
> head(iris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}
\end{framed}
We will only consider the numeric variables in the cluster analysis. As mentioned previously, there are two functions to compute the distance matrix: dist and daisy. It should be mentioned that for data that's all numeric, using the function's defaults, the two methods will give the same answers. We can demonstrate this as follows:
\begin{framed}
\begin{verbatim}
> iris.use = subset(iris,select=-Species)
> d = dist(iris.use)
> library(cluster)
> d1 = daisy(iris.use)
> sum(abs(d - d1))
[1] 1.072170e-12
\end{verbatim}
\end{framed}
Of course, if we choose a non-default metric for dist, the answers will be different:
\begin{framed}
\begin{verbatim}
> dd = dist(iris.use,method='manhattan')
> sum(abs(as.matrix(dd) - as.matrix(d1)))
[1] 38773.86
\end{verbatim}
\end{framed}
The values are very different!
Continuing with the cluster example, we can calculate the cluster solution as follows:
\begin{framed}
\begin{verbatim}
> z = agnes(d)
\end{verbatim}
\end{framed}
The plotting method for agnes objects presents two different views of the cluster solution. When we plot such an object, the plotting function sets the graphics parameter ask=TRUE, and the following appears in your R session each time a plot is to be drawn:
Hit <Return> to see next plot: 

If you know you want a particular plot, you can pass the which.plots= argument an integer telling which plot you want.
The first plot that is displayed is known as a banner plot. The banner plot for the iris data is shown below:

The white area on the left of the banner plot represents the unclustered data while the white lines that stick into the red are show the heights at which the clusters were formed. Since we don't want to include too many clusters that joined together at similar heights, it looks like three clusters, at a height of about 2 is a good solution. It's clear from the banner plot that if we lowered the height to, say 1.5, we'd create a fourth cluster with only a few observations.
The banner plot is just an alternative to the dendogram, which is the second plot that's produced from an agnes object:

The dendogram shows the same relationships, and it's a matter of individual preference as to which one is easier to use.
Let's see how well the clusters do in grouping the irises by species:
\begin{framed}
\begin{verbatim}
> table(cutree(z,3),iris$Species)
   
    setosa versicolor virginica
  1     50          0         0
  2      0         50        14
  3      0          0        36
\end{verbatim}
\end{framed}
We were able to classify all the setosa and versicolor varieties correctly. The following plot gives some insight into why we were so successful:
\begin{framed}
\begin{verbatim}
> splom(~iris,groups=iris$Species,auto.key=TRUE)
\end{verbatim}
\end{framed}
